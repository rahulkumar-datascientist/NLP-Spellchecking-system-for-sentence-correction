{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [],
   "source": [
    "lines = open(\"holbrook.txt\").readlines()                            # read the txt file into lines\n",
    "data = []                                                           # a list of dictionary objects\n",
    "\n",
    "clean_lines = [line.strip() for line in lines]                      # removing the leading and trailing spaces\n",
    "non_empty_lines = [line for line in clean_lines if line!= '']       # removing any empty lines\n",
    "\n",
    "for line_no in non_empty_lines:\n",
    "    original = []                                                   # creating arrays to store respective \n",
    "    corrected = []                                                  # data\n",
    "    indexes = []                                                    \n",
    "    line_token = word_tokenize(line_no)                             # splitting each word of a line into tokens\n",
    "    for line_value in line_token:\n",
    "        if(\"|\" in line_value):                                      # if token contains '|' , find the respective\n",
    "            indexes.append(line_token.index(line_value))            # index and append it to index[].\n",
    "            w_c = line_value.split(\"|\")                             # split the token as <original>|<corrected>\n",
    "            original.append(w_c[0])                                 # and append respectively to original[] and\n",
    "            corrected.append(w_c[1])                                # corrected[]\n",
    "        else:\n",
    "            original.append(line_value)                             # otherwise, add token to both original[] and\n",
    "            corrected.append(line_value)                            # corrected[]\n",
    "\n",
    "    # for each line, create a dictionary \n",
    "    dict_value = {'original':original,'corrected':corrected,'indexes':indexes}\n",
    "    data.append(dict_value)                                         # append the dictionary to data[]                                        \n",
    "    \n",
    "#data[36]\n",
    "\n",
    "assert(data[2] == {\n",
    "   'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "   'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corrected_word_list = []                        # list of all tokens present in corrected training sentences\n",
    "\n",
    "# collect all the corrected token(lower case) from training into 1 list\n",
    "for train_line_no in range(len(train)):         \n",
    "    corrected_word_list.append([train_word.lower() for train_word in train[train_line_no]['corrected']])\n",
    "corrected_word_list = [item for sublist in corrected_word_list for item in sublist]\n",
    "\n",
    "def unigram(word):\n",
    "    freq_words = Counter(corrected_word_list)           # create a dictionary with:  \n",
    "    return (freq_words[word])                           # key = token and value = frequency of occurance\n",
    "\n",
    "def prob(word):\n",
    "    return(unigram(word)/len(corrected_word_list))      # find probability of a token : freq(token)/total no of token\n",
    "\n",
    "#Test your code with the following\n",
    "assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1536070558621,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance('hello','hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HoilAmFW8PCb"
   },
   "outputs": [],
   "source": [
    "all_train_word_list = list(set(corrected_word_list)) # all unique tokens from complete list of **correct** words\n",
    "                                                     # not using original words, because it will give distance of 0\n",
    "                                                     # and if there are large no. of those wrong words in training set \n",
    "                                                     # the probability of those words will increase, resulting in wrong\n",
    "                                                     # word to be selected.\n",
    "def get_candidates(token):\n",
    "    dist = dict()\n",
    "        \n",
    "    for set_element in all_train_word_list:                     # creating a dictionary with key = unique token     \n",
    "        dist[set_element] = edit_distance(token,set_element)    # value = distance b/w unique token and argument token\n",
    "    \n",
    "    res_word = [word for word,distance in dist.items() if distance == min(dist.values())]    \n",
    "    return(sorted(res_word,reverse=True))                       # returning a list of tokens having least distance\n",
    "    \n",
    "#print(\".\" in all_train_word_list)\n",
    "    \n",
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [],
   "source": [
    "def correct(sentence):    \n",
    "    op_sentence = []                                   # list containing the corrected output of sentence\n",
    "    \n",
    "    for sentence_word in sentence:                           \n",
    "        if sentence_word in all_train_word_list:       # if token is present in training set list\n",
    "            op_sentence.append(sentence_word)          # append it to output list\n",
    "        else:                                           \n",
    "            candidate_dist_prob = dict()                                         # else create a dictionary with  \n",
    "            for candidate_element in get_candidates(sentence_word):              # key = candidate token, \n",
    "                candidate_dist_prob[candidate_element] = prob(candidate_element) # value = probability of occurance\n",
    "            \n",
    "            for candidate,probb in candidate_dist_prob.items():\n",
    "                if probb == max([x for x in candidate_dist_prob.values()]):      # select candidate with max \n",
    "                    op_sentence.append(candidate)                                # probability, append to output list\n",
    "\n",
    "    return(op_sentence)                                                          # return the output list\n",
    "\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1536071822989,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.40402356176448 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    test_sentence_list_all = []                   # list of tokens from original sentences of test\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    # collect all the original sentences(lower case) from test into 1 list\n",
    "    for test_line_no in range(len(test)):\n",
    "        test_sentence_list_all.append([ele.lower() for ele in test[test_line_no]['original']])\n",
    "\n",
    "    # for each sentence in list, find the total number of matching tokens with the list returned by correct() \n",
    "    # calculate the accuracy \n",
    "    for sentence_no in test_sentence_list_all:   \n",
    "        match_case = len([i for i, j in zip(sentence_no, correct(sentence_no)) if i == j])\n",
    "        accuracy += float(match_case/len(sentence_no))\n",
    "    \n",
    "    return((accuracy/len(test_sentence_list_all))*100)\n",
    "\n",
    "print(accuracy(test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bigrams instead of unigrams and calculating PMI \n",
    "\n",
    "By using **Bigrams model** we can understand the context information of the token. It calculates the conditional probability of a token given the preceding token. By calculating bigram frequency we can determine better which token is preceeded by which word most of the times. Also, by using **PMI** we can strengthen our findings. PMI tells us which pair of bigram has high co-relation measure and provides a better understanding.\n",
    "\n",
    "**steps taken to improve the algorithm**\n",
    "\n",
    "1) create a list with all the corrected training tokens after removing the punctuation. \n",
    "\n",
    "2) create a dictionary with key = bigrams and value = PMI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_word_list = [item for item in all_train_word_list if item not in string.punctuation]\n",
    "\n",
    "PMI_bigrams = dict()\n",
    "Bigram_measure_PMI = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(new_train_word_list)\n",
    "for bigram, pmi in finder.score_ngrams(Bigram_measure_PMI.pmi):\n",
    "    PMI_bigrams[bigram] = pmi\n",
    "\n",
    "#PMI_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) create a new list having stem of tokens. **(stemming)**\n",
    "\n",
    "This is done in order to check wether the word's stem is present in list or not.\n",
    "\n",
    "**takes a little while to execute (~5 mins)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# all_train_word_list = list(set(all_train_word_list))\n",
    "for set_element in new_train_word_list:\n",
    "    all_train_word_list_stem = [porter_stemmer.stem(x) for x in new_train_word_list]\n",
    "\n",
    "\n",
    "#all_train_word_list_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) we check each word in sentence with the corrected training tokens.\n",
    "\n",
    "     4.1) If the word is present: we add that to our output \n",
    "\n",
    "     4.2) else: we check wether the stem of word is present in the list of stems.\n",
    "    \n",
    "         4.2.1) if present, we add the word in output. \n",
    "    \n",
    "         4.2.2) else:\n",
    "                * we find candidates for the word.\n",
    "                * create a sequence (previous word, candidate_word)\n",
    "                * find the PMI value for sequence\n",
    "                * store the results in dictionary where, key = candidate and value = PMI measure\n",
    "                * select the candidate with highest PMI measure\n",
    "                * add the candidate to output\n",
    "\n",
    "**why are we checking the word in list of stem of words ?**\n",
    "\n",
    "If the list has a matching stem, we don't change the word (it means the word is not wrong, just that our training list  dosent have the exact match of the word)\n",
    "\n",
    "else, we find the candidate of the wrong_word and replace it with word having highest PMI based on sequence formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_new(sentence):    \n",
    "    op_sentence = []                          # output list having corrected words\n",
    "    \n",
    "    for sentence_word_index in range(len(sentence)):                       # check 'word' is present in training\n",
    "        if sentence[sentence_word_index] in new_train_word_list:           # list. if present, append it to \n",
    "            op_sentence.append(sentence[sentence_word_index])              # output list\n",
    "        else:\n",
    "            if (porter_stemmer.stem(sentence[sentence_word_index]) in all_train_word_list_stem):\n",
    "                op_sentence.append(sentence[sentence_word_index])          # if not, check if stem of word is present\n",
    "                                                                           # if present, append it to output list\n",
    "            else:\n",
    "                candidate_dist_prob = dict()                               \n",
    "                for candidate_element in get_candidates(sentence[sentence_word_index]):\n",
    "                    sequence = (sentence[sentence_word_index-1],candidate_element)  # sequence(previous word,candidate)\n",
    "                    try:\n",
    "                        candidate_dist_prob[candidate_element] = PMI_bigrams[sequence] # find the PMI measure\n",
    "                    except:\n",
    "                        candidate_dist_prob[candidate_element] = 0\n",
    "                               \n",
    "                for candidate,probb in candidate_dist_prob.items():                     # select candidate having \n",
    "                    if probb == max([x for x in candidate_dist_prob.values()]):         # highest PMI measure\n",
    "                        op_sentence.append(candidate)\n",
    "                        break\n",
    "                    \n",
    "    return(op_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.6912181290406 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy_new(test):\n",
    "    test_sentence_list_all = []\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # collect all the original sentences(lower case) from test(removing punctuations) into 1 list\n",
    "    for test_line_no in range(len(test)):\n",
    "        test_sentence_list = [ele.lower() for ele in test[test_line_no]['original']if ele not in string.punctuation]\n",
    "        if len(test_sentence_list)== 0:\n",
    "            continue\n",
    "        test_sentence_list_all.append(test_sentence_list)\n",
    "    \n",
    "    # for each sentence in list, find the total number of matching tokens with the list returned by correct_new() \n",
    "    # calculate the accuracy \n",
    "    for sentence_no in test_sentence_list_all:\n",
    "    \n",
    "        match_case = len([i for i, j in zip(sentence_no, correct_new(sentence_no)) if i == j])\n",
    "        accuracy += float(match_case/len(sentence_no))\n",
    "    \n",
    "    return((accuracy/len(test_sentence_list_all))*100)\n",
    "\n",
    "print(accuracy_new(test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CT5120/CT5146 - Assignment 1",
   "provenance": [
    {
     "file_id": "12crGPce24mcgITZPs7hU_9CPnLAcyIq6",
     "timestamp": 1603097790764
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
